# 2018.06.28

- SEO란 무엇인가? (Google, Naver)
- React SPA에서 SEO 하려면? (HTML, CSS, JavaScript의 관점에서)
- Server-side rendering & Prerendering
- Next.js 소개
- SEO는 항상, 모든 부분에 필요한가?

서버사이드 렌더링이 왜 필요한가? (SEO)
주니어 단계에서 서버사이드 렌더링을 직접 세팅할 일은 없지만 서버 사이드 렌더링이 어떤 느낌인지 **왜 필요한지**를 알아보자

## 1. SEO

Search Engine Optimization: 검색 엔진 최적화

프론트엔드 개발자들이 잘 해야하는 영역 중 하나이다.  
많은 검색엔진들이(Google, Naver, Bing) 있다. 많은 웹사이트들의 정보를 어떻게든 가져와서 자신들의 검색엔진에 쌓아놓을 것이다. 이러한 <sup>1</sup>**정보를 가져오는 방식**이 검색엔진마다 조금씩 다르다. 

정보를 가져오는 방식뿐만 아니라 <sup>2</sup>**어떤 웹사이트를 상위에 띄워줄 것 인가**하는 알고리즘도 다르다. 

검색엔진이 우리 웹 사이트를 잘 가져와서 상위에 띄워주도록 하는 것을 **SEO**라고 한다.

> 2번의 방식은 잘 공개하지 않는다.  
구글의 경우 다른 사이트들로부터 많이 링크가 걸린 사이트를 상위에 노출한다고 공개했지만 정확한 알고리즘을 공개하지는 않았다.(정확한 알고리즘을 공개하면 돈과 직결되는 문제이며 어뷰징(악용) 문제가 발생할 수 있어 공개하지 않는다.)  
SEO 전문으로 하는 전문가/마케터들이 보통 2번을 담당한다.

정보를 검색엔진에 어떻게 가져와서 저장하는지는 공개하고 있다.

+ [Google에서 이해할 수 있는 메타태그](https://support.google.com/webmasters/answer/79812?hl=ko)
+ [웹 표준 최적화 기본 가이드](https://webmastertool.naver.com/guide/basic_optimize.naver)

robots.txt가 가장 상위에 있으면 웹사이트의 검색엔진이 정보를 긁어가도 되는지 여부를 확인할 수 있다.

```txt
User-agent: *
Disallow: /version/
```

- User-agent : 요청을 보낸 프로그램이 무엇인지 (구글 검색 봇 같은 거)
  *는 어떤 검색엔진이 와도 허용하겠다는 의미  
  예를 들어 네이버 검색엔진만 허용한다고 하면 다음과 같음
  ```txt
  User-age: Yeti
  Allow: /
  ```
- Disallow: 접근 허용하지 않는 페이지

robots.txt를 프로젝트에 추가해서 build하자

> 여러분의 사이트가 네이버 검색로봇의 접근을 허용하고 있는지 robots.txt 와 방화벽 설정을 다시 한번 확인해 주세요.
메인 페이지의 meta 태그에 noindex 처리가 되어있다면 해제해주세요. noindex가 표기된 페이지는 검색 반영에서 제외됩니다.

검색엔진은 robots.txt뿐만 아니라 `<meta />`도 확인한다.  
meta 태그가 없다면 그냥 모든 html파일, 이미지등을 긁어간다.

검색엔진은 우리의 웹사이트를 어떻게 방문하는 걸까??  
우리 웹사이트의 정보를 가져가려면 **우리 웹사이트에 요청을 보내야한다**. 이러한 웹사이트에 요청을 보내는 기계를 **크롤러**라고 한다. postman같은 것인데 자동화가 되어 있다고 생각해보자.  

#### [예제] 검색엔진이 되어서 웹사이트의 정보를 긁어와 보자

+ https://boring-bell-7c476f.netlify.com/posts  

검색엔진은 로그인 기능이 없다.  
react처럼 js로 화면이 렌더되는 경우 검색엔진이 웹사이트의 정보를 긁어갈 수 없다.

구글의 경우 postman같이 요청을 보내고 자바스크립트를 실행시키는 경우도 있다.  
단, react-router의 경우는 잘 가져가지 못한다.  
예를들어 쇼핑몰이라면 각 상품을 검색엔진에 노출해야 하는데, 검색엔진에 걸리지 않는 큰 문제가 발생한다.

서버사이드 렌더링은 바로 이러한 문제점때문에 사용한다.  
JS해석 못하는 크롤러가 요청을 보내도 정보를 잘 보여주는 것

서버사이드 렌더링이 적용된 사이트에 postman으로 요청을 보내보자(https://reactjs.org/)

## 2. server-side rendering(SSR)

서버 사이드 렌더링을 하려면 어떤 처리들이 필요할까

검색엔진이 우리 웹사이트로 요청을 보낸다.  
웹사이트가 React같은 걸 이용해 만들었다면 JS를 해석해서 html파일을 만들어 응답해야 한다.

원래 JS해석은 브라우저에서 해주면 되었는데, 검색엔진의 요청을 받으면 서버에서 JS를 해석해줘야 하는 상황이 발생한다. 예를들면 Node.js에서 JS를 해석해서 응답을 보내줘야하는 상황이 되는 것이다. 이럴 경우 JS를 해석하는 일을 서버와 브라우저 두 곳에서 하도록 만들어야 한다.

JSON서버 같은 서버를 쓰고 있다고 하면, API가 있다고 하면 
+ 브라우저에서 서버에 요청한다.
+ 서버가 API에 데이터를 받아온다.
+ 브라우저에 정보를 던진다.
+ 브라우저에 표시된 웹사이트의 링크를 클릭하면 브라우저에서 API에 요청을 보내야한다.
(난장판...)
**원래 브라우저에서 하던일을 서버에서도 해줘야한다.**

Creat-react-app을 가지고 직접 서버사이드렌더링 세팅을 할 수 있다. 단, 엄청나게 복잡하고 신경써야할 부분이 굉장히 많아진다.  
직접 서버사이드 렌더링을 짜기보다는 미리 짜놓은 라이브러리를 이용한다.

## 3. Next.js

+ https://github.com/zeit/next.js/
+ [next.js는 어떻게 동작하는가?](https://blueshw.github.io/2018/04/15/why-nextjs/)

서버사이드렌더링을 해주는 어플리케이션

netlify는 올려놓은 파일을 브라우저에 제공하는 서비스이다. 
요청에 따라 다른 응답을 보내주는 것을 할 수는 없다. (build를 하기는 하지만...)
서버사이드 렌더링을 지원하려면 직접 프로그램을 돌릴 수 있는 서버가 필요하다.  
(그래서 실습은 하지만 프로젝트에 쓰긴 어렵다...)

웹사이트 운영시 next.js를 다음과 같이 돌릴 수 있어야 한다.
```
npm run build
npm start
```

### 3.1 CSS 처리

서버사이드 렌더링을 위해 CSS 처리도 필요한다. 독자적인 CSS 제공 방식을 가지고 있다.(https://github.com/zeit/next.js/#css)

next.js에 다음과 같이 엘리먼트 선택자를 써도 전역적으로 적용되는 것이 아니라 해당 컴포넌트에만 먹힌다.  
스타일 태그의 스코프를 지원한다.
```
export default () =>
  <div>
    Hello world
    <p>scoped!</p>
    <style jsx>{`
      p {
        color: blue;
      }
      div {
        background: red;
      }
      @media (max-width: 600px) {
        div {
          background: blue;
        }
      }
    `}</style>
    <style global jsx>{`
      body {
        background: black;
      }
    `}</style>
  </div>
```

vscode에서 스타일 태그 부분을 예쁘게 보여주는 styled jsx라는 확장프로그램도 있다.
- vscode-styled-jsx

### 3.2 Populating

helmet이라는 라이브러리와 비슷함  
페이지들마다 다른 타이틀과 메타태그를 제공하기 위한 기능

### 3.3. Fetching data and component lifecycle

next.js는 자체 라이프사이클 메서드를 가지고 있다.

```js
import React from 'react'

export default class extends React.Component {
  static async getInitialProps({ req }) {
    const userAgent = req ? req.headers['user-agent'] : navigator.userAgent
    return { userAgent }
  }

  render() {
    return (
      <div>
        Hello World {this.props.userAgent}
      </div>
    )
  }
}
```
어떤 작업을 통해 객체를 반환하면 해당 페이지 컴포넌트의 prop이 된다.
서버에서 getInitialProps를 통해 API에서 정보를 받아와 브라우저에 만든 html을 보내준 것

### 3.4 Routing

creat-react-app으로 만들어진 웹 사이트의 경우 어떤 경로로 들어와도 똑같은 html을 보여주면 되었는데 이제는 다른 경로로 들어오면 다른 html 페이지를 보여줘야 한다.

서버사이드 렌더링을 할 때와 안할 때의 처리가 다른다. 
리액트 라우터에서 쓰는 방식과 next에서 쓰는 방식을 동시에 제공할 수 있는 무언가가 있어야 한다.(서버사이드렌더링과 클라이언트 측 라우팅 둘다 제공)

익스프레스에서 라우팅은 서버로 어떤 경로로 들어오는지에 따라 다른 응답을 주도록 했었다. next.js역시 그런 기능을 하고 있다.

리액트 앱처럼 동작하게(새로고침 없이 pushState로) 하는 방식 역시 필요

next.js는 리액트 라우터를 쓰지 못하고 내장된 별도의 라우터를 사용한다. 
(css, 라이프사이클, head관련 컴포넌트, 라우터를 별도로 가지고 있다. 이런 복잡한 작업을 미리 구현한 것이 next.js이다.)

next.js에 내장된 Link 컴포넌트
```js
// pages/index.js
import Link from 'next/link'

export default () =>
  <div>
    Click{' '}
    <Link href="/about">
      <a>here</a>
    </Link>{' '}
    to read more
  </div>
```

Next.js가 독자적으로 갖고있는 기능을
- CSS
- head관련 컴포넌트
- 라이프사이클
- 라우터

## 4. SEO는 항상, 모든 부분에 필요한가?

SSR을 사용하는 이유
+ 검색엔진에게 정보를 주기위해 사용한다. 
+ html문서가 바로 브라우저에 전달하므로 브라우저가 해석할 필요없이 바로 보여줄 수 있다.

서버사이드 렌더링의 단점은 - 서버에서 화면을 렌더링하기 위한 시간이 걸린다. 

모든 웹사이트가 검색엔진에 걸려야할까?
예를들어 트렐로같은 개인화된 웹사이트는 SSR을 할 필요가 없다.

게시판같은 누구나 볼 수 있는 정보는 개인화된 서비스가 아니나 할 일 관리같은 경우는 개인화된 서비스다. 

쇼핑몰같이 검색엔진에 걸려야하는 경우는 거의 필수적으로 SSR이 필요한다. Air bnb라거나 많은 웹사이트에서 SSR을 지원한다.

단, 할일 관리 서비스의 홍보를 위한 소개페이지처럼 부분적으로 서버사이드 렌더링이 필요한 경우가 있다. 그럴경우 두개를 분리해서 따로 제공한다.
hexo, jekyll 같은 정적 웹사이트를 만들어 검색엔진에 걸리게 한다.  
netlify사이트도 그렇게 제공하고 있다.
+ https://www.netlify.com/
+ https://app.netlify.com/ 

만들고자 하는 서비스가 개인화된 서비스인지 아닌지를 처음에 결정해서 초기부터 서버사이드 렌더링 작업을 해야한다.

리액트 헬멧은 next.js를 사용하지 않아도 똑같은 기능을 제공한다. (페이지마다 다른 head를 보여준다.)

서버사이드 렌더링하지 않고도 검색엔진에 걸리게 하는 방법이 있다. (Prerendering)

### 4.1. Prerendering

+ https://www.netlify.com/docs/prerendering/
+ https://prerender.io/

서버사이드 렌더링은 검색엔진의 요청에 내용이 채워진 HTML을 보내야 하고 서버 환경/브라우저 환경이 달라서 두가지 상황에 따라 언떤 동작을 할지 조건을 걸 수 밖에 없어서 일이 복잡해진다.  
Prerendering 방식은 별도의 서버를 두고 검색엔진을 위한 브라우저를 제공해 그 프로그래밍을 할 수 있는 브라우저에서 JS해석된 파일을 검색엔진에 전달하는 것이다.  
문제점은 어떤 시점의 html을 검색엔진에 넘겨줄 것인가에 대한 정보가 설정되어 있어야 한다.  
아직 next.js같은 많은 시도 끝에 만들어진 좋은 솔루션은 없다.


---

## 참고링크

+ https://it-consultis.com/blog/best-seo-practices-for-react-websites
+ https://jamstack.org/
+ https://medium.freecodecamp.org/using-fetch-as-google-for-seo-experiments-with-react-driven-websites-914e0fc3ab1
